#summary How to customize your crawler

= What is a Journal? =
Journal is by default a very simple class, which can be modified quite extensive to produce a complex set of rules for the crawler to follow. You can either subclass the default `Journal`-class or you can create a new object with the three methods required by the crawler (see below).

== How it works ==
To create an instance of the `WebCrawler`-class you need to pass it a Journal-like object. The crawler will save this object internally and use it to call back every time a user-made decision or action is required. These decisions are:

 * *How many spiders should live?*
 When starting the crawler using `WebCrawler.start()` the `spiders`-method of the `Journal`-object is revoked by the crawler. This method should return the maximum amount of spider-threads alive at a time. All non-integer values returned will raise an exception.

 * *What URL should I crawl next?*
 Every time the crawler is ready to crawl a new URL, the `nextUrl`-method of the `Journal`-object is revoked. This method should either return the next URL to crawl, or `None`. Sometimes it's necessary to return `None` (eg. when starting with a single URL, no more URLs will be available before the first page has been downloaded and parsed). The crawler will stop crawling when there is no spiders left alive and `nextUrl()` returns `None`.

 * *Parsing the page*
 Every time the crawler has finished downloading a page, the `parse`-method of the `Journal`-object is revoked by the crawler with a `HTMLDocResult`-object as only parameter. This object holds a couple of methods and members to extract some basic data from the page, like title, meta-elements and links. For example, using `HTMLDocResult.getLinks()` will return a list of links found on the page (relative URLs is converted to fully qualified URLs). The Journal must keep track of these links and give them back to the crawler through `Journal.nextUrl()`.

== The default Journal-object ==
The default `Journal`-object implements (probably) the easiest way of feeding URLs to the crawler and keeping track of URLs already crawled. It holds three private members for storing URLs:

 * A list of URLs not yet crawled
 * A list of URLs being crawled right now
 * A list of URLs already crawled

When the crawler revokes the `Journal.nextUrl()` method, the default `Journal`-object will simply pop and return the first element of the list of URLs not yet crawled. This URL will be appended to the list of URLs being crawled right now.

When the crawler then revokes the `Journal.parse()` method, the default `Journal`-object will then remove the URL from the list of URLs being crawled right now, and append it to the list of URLs already crawled. By using the `HTMLDocResult.getLinks()` method to extract all links found on the page, `Journal` will save the links using `self.add(url)`.

The default `Journal`-object also implements two methods to save and reload progress info: `Journal.dumpState()` and `Journal.loadState(s)`. `dumpState()` will simply pickle the three above mentioned lists and return the pickled string, while `loadState(s)` will simply unpickle the string given and save the data into `self`.

== The Journal object ==
When creating a `WebCrawler`-object one must pass it a Journal-like object. The crawler uses this object to ask for URLs to crawl and callback with pages processed (including url, HTML-source and HTTP-status code). Subclassing `Journal()` is not necessary, but the same methods must be present or exceptions will be raised during runtime.

*`spiders() -> Integer`*
  Revoked every time the crawler is started using `WebCrawler.start()`. Should return the max. amount of spiders alive at a time

*`addUrl(str url, str referer)`*
  Revoked by the crawler whenever a new URL was found. `url` is the link found. `referer` is URL of the page where `url` was found. It is the Journal's job to keep track of URLs that have already been visited, so identical links may be given multiply times. `addUrl()` should not return anything.

*`getNextUrl() -> String|None`*
  Revoked by the crawler whenever a new URL is needed. The URL returned will be the next to be downloaded. This method can return `None` if there aren't any URLs available (eg. when starting with a single URL, there won't be more URLs available before the first page has been downloaded and parsed).

*`urlCrawled(str url, str source, int status)`*
  Revoked by the crawler whenever an URL has been downloaded and links extracted from it. Any links found in the source is added through `addUrl()` BEFORE this method is revoked. When a connection error occurs (`urllib2.URLError`) both `source` and `status` is `None`. `urlCrawled()` should not return anything.

*`dumpState() -> String|None`*
  Revoked by the crawler when an exception occurs during runtime. It should return a pickle (or String-like) object representing the current state of the crawler. This string will be saved into a file, which can be loaded back into the Journal using `loadState()` to continue where progress ended. When `None` is returned, state is not being saved.

*`loadState(str state)`*
  Given a string `state` (returned by `dumpState()`), this method should re-load progress data back into selv. `loadState()` should not return anything.