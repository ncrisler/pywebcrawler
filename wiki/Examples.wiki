#summary Examples on how to use and customize your crawler

<wiki:toc max_depth="2" />

== Basic usage ==
The crawler in it's least complex form:
{{{
from pywebcrawler import WebCrawler, Journal

journal = Journal()
journal.addUrl('http://www.start-here.com/')
journal.addUrl('http://www.also-visit-me.com/')

try:
    crawler = WebCrawler(journal)
    crawler.start()
except KeyboardInterrupt:
    crawler.stop()
}}}


== Crawling specific domain(s) ==
When validation on URLs are required, adding the logic to the `addUrl`-method of the [Journal Journal-object] would in most cases be preferable. Heres a simple example showing how to subclass the Journal object and modify the `addUrl`-method to fit your purpose. [URLs Click here to read more about how the crawler works with URLs].
{{{
from pywebcrawler import WebCrawler, Journal, getDomainName

class CustomJournal(Journal):
    def addUrl(self, url, referer):
        if getDomainName(url) == 'crawl-me.com':
            Journal.addUrl(self, url, referer)

journal = CustomJournal()
journal.addUrl('http://www.crawl-me.com/')  # Will be crawled
journal.addUrl('http://crawl-me.com/test')  # Will be crawled
journal.addUrl('http://test.crawl-me.com/') # Ignored
journal.addUrl('http://www.tgdaily.com/')   # Ignored

try:
    crawler = WebCrawler(journal)
    crawler.start()
except KeyboardInterrupt:
    crawler.stop()
}}}

== Saving the HTML source ==
Whenever a page has been downloaded and parsed, the `urlCrawled`-method of the [Journal Journal-object] is revoked by the crawler, passed the URL, HTML-source and a status code.
{{{
from pywebcrawler import WebCrawler, Journal, getDomainName

class CustomJournal(Journal):
    def urlCrawled(self, url, source, status):
        Journal.urlCrawled(self, url, source, status)

        # When an urllib2.URLError occurs, both source and status will be None.
        # In that case, we don't have anything to save.
        if not source is None:
            self._save(url, source)
    
    def _save(self, url, source):
        import md5
        m = md5.new(url)
        filename = '%s.html' % m.hexdigest()
        open(filename, 'w').write(source)

journal = CustomJournal()
journal.addUrl('http://www.crawl-me.com/')

try:
    crawler = WebCrawler(journal)
    crawler.start()
except KeyboardInterrupt:
    crawler.stop()
}}}

== Saving progress if an error occurs ==
If an unexpected exception is raised during runtime, much information about progress may be lost. Therefore the [Journal Journal-object] implements two simple methods to overcome this: `dumpState()` and `loadState()`. In the example below the current progress will be save into progress.txt if an unexpected exception occurs. You can pass the content of progress.txt into `loadState()` to continue from where the crawler failed.
{{{
from pywebcrawler import WebCrawler, Journal

journal = CustomJournal()
journal.addUrl('http://www.crawl-me.com/')

try:
    crawler = WebCrawler(journal)
    crawler.start()
except KeyboardInterrupt:
    # Stopped by user - just quit
    crawler.stop()
except:
    # Unexpected exception occurred - save state.
    stateStr = journal.dumpState()
    open('progress.txt', 'w').write(stateStr)
}}}