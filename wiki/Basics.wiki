#summary A basic introduction to pywebcrawler

<wiki:toc max_depth="1" />

= Journal = 
What is a Journal?

== Examples of customized Journals ==
Examples...

<br>
= Request objects =
What is a Request object?

== Members ==
Public members of the Request object:

*`url`*
  URL to request, excluding query string.
*`method`*
  HTTP request method
*`args`*
  HTTP query as dictionary

== Methods ==
Public methods of the Request object:

*`__init__(url, method='GET', args={})`*
  Initialize the Request object
*`addHeader(name, value)`*
  Add a header to the HTTP request.
*`addCookie(name, value, **attrs)`*
  Add a cookie to the HTTP request.

<br>
= Response objects =
What is a Response object?

== Methods ==
Public methods of the Response object:

*`getCookies()`*
  Returns a list of cookies set by the server
*`getCookie(name, default=None)`*
  Returns the value of a cookie set by the server. Returns `default` when cookie name doesn't exist.
*`getTitle()`*
  Returns contents of the first occurrence of a `<TITLE>` tag, or `None` if no title-tag was found on the page.
*`getMeta()`*
  Returns a list of dictionaries, where each represents the attributes of a `<META>` tag found on the page
*`getLinks()`*
  Iterate over links found on the page. Yields `(target, tag, attributes)`. 
  Example: {{{('http://some-url.com/some-link.html', 'a', {'target':'_blank'})}}}

<br>
= URLs =
pywebcrawler is using the *lxml* module fetch links from HTML pages. lxml uses the URL in which a link was found as base URL, and pays attention to `<base href>` tags when converting relative to absolute URLs.