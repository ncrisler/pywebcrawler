#summary A basic introduction to pywebcrawler

<wiki:toc max_depth="2" />

= Journal = 
What is a Journal?

== Examples of customized Journals ==
Examples...

<br>
= Request objects =
What is a Request object?

== Members ==
Public members of the Request object:

*`str url`*
  URL to request, excluding query string.
*`str method`*
  HTTP request method
*`dict args`*
  HTTP query as dictionary

== Methods ==
Public methods of the Request object:

*`__init__(url, method='GET', args={})`*
  Initialize the Request object
*`addHeader(name, value)`*
  Add a header to the HTTP request.
*`addCookie(name, value, **attrs)`*
  Add a cookie to the HTTP request.

<br>
= Response objects =
What is a Response object?

== Members ==
Public members of the Request object:

*`str url`*
  The requested URL
*`str html`*
  Response HTML. Is `None` when the page could not load (socket error)
*`dict headers`*
  HTTP headers returned by the server
*`str msg`*
  HTTP message returned by the server
*`int code`*
  HTTP status code returned by the server

== Methods ==
Public methods of the Response object:

*`getCookies()`*
  Returns a list of cookies set by the server
*`getCookie(name, default=None)`*
  Returns the value of a cookie set by the server. Returns `default` when cookie name doesn't exist.
*`getTitle()`*
  Returns contents of the first occurrence of a `<TITLE>` tag, or `None` if no title-tag was found on the page.
*`getMeta()`*
  Iterate over dictionaries, where each represents the attributes of a `<META>` tag found on the page.
*`getLinks()`*
  Iterate over links found on the page. Yields `(target, tag, attributes)`.<br>
  Example: {{{('http://some-url.com/some-link.html', 'a', {'target':'_blank'})}}}.

<br>
= URLs =
pywebcrawler is using the *lxml* module fetch links from HTML pages. lxml uses the URL in which a link was found as base URL, and pays attention to `<base href>` tags when converting relative to absolute URLs.